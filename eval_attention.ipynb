{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4da006de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "GPU Memory: 8.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model_attention import Encoder, Decoder, Seq2Seq\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from data_loader import load_data, create_tokenizers, TranslationDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd7a3d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention, source, target, src_vocab, trg_vocab):\n",
    "    # Convert attention weights to numpy array\n",
    "    attention = attention.squeeze(1).cpu().numpy()\n",
    "    \n",
    "    # Get source and target tokens\n",
    "    src_tokens = [list(src_vocab.keys())[list(src_vocab.values()).index(i)] for i in source if i not in [src_vocab['<sos>'], src_vocab['<eos>'], src_vocab['<pad>']]]\n",
    "    trg_tokens = [list(trg_vocab.keys())[list(trg_vocab.values()).index(i)] for i in target if i not in [trg_vocab['<sos>'], trg_vocab['<eos>'], trg_vocab['<pad>']]]\n",
    "    \n",
    "    # Create figure and axis\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    # Plot attention matrix\n",
    "    cax = ax.matshow(attention, cmap='viridis')\n",
    "    fig.colorbar(cax)\n",
    "    \n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + src_tokens, rotation=90)\n",
    "    ax.set_yticklabels([''] + trg_tokens)\n",
    "    \n",
    "    # Show every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    \n",
    "    plt.title('Attention Map')\n",
    "    plt.xlabel('Source Tokens')\n",
    "    plt.ylabel('Target Tokens')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('attention_map.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19f9c890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, iterator, target_tokenizer, device):\n",
    "    model.eval()\n",
    "    translations = []\n",
    "    references = []\n",
    "    attention_maps = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            src = batch['source'].to(device)\n",
    "            trg = batch['target'].to(device)\n",
    "            \n",
    "            output = model(src, trg, 0)  # Turn off teacher forcing\n",
    "            \n",
    "            # Get the predicted tokens\n",
    "            pred_tokens = output.argmax(2)\n",
    "            \n",
    "            # Convert to text\n",
    "            for i in range(len(pred_tokens)):\n",
    "                # Get the tokens\n",
    "                pred_seq = pred_tokens[i].cpu().numpy()\n",
    "                ref_seq = trg[i].cpu().numpy()\n",
    "                \n",
    "                # Convert to text\n",
    "                pred_text = target_tokenizer.decode(pred_seq)\n",
    "                ref_text = target_tokenizer.decode(ref_seq)\n",
    "                \n",
    "                translations.append(pred_text)\n",
    "                references.append(ref_text)\n",
    "                \n",
    "                # Store attention maps if available\n",
    "                if hasattr(model, 'attention_weights'):\n",
    "                    attention_maps.append((src[i].cpu().numpy(), pred_seq, model.attention_weights[i].cpu().numpy()))\n",
    "    \n",
    "    return translations, references, attention_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45c63b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading data from english_assamese.csv\n",
      "Total samples: 87849\n",
      "Training samples: 70279\n",
      "Validation samples: 17570\n",
      "Creating tokenizers...\n",
      "Training English tokenizer...\n",
      "Training Assamese tokenizer...\n",
      "Tokenizers created successfully!\n",
      "Model loaded successfully from checkpoint\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 61\u001b[0m\n\u001b[0;32m     54\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m     55\u001b[0m     val_dataset,\n\u001b[0;32m     56\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[0;32m     57\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     58\u001b[0m )\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m translations, references, attention_maps \u001b[38;5;241m=\u001b[39m evaluate_model(model, val_loader, target_tokenizer, device)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Calculate BLEU score\u001b[39;00m\n\u001b[0;32m     64\u001b[0m bleu_score \u001b[38;5;241m=\u001b[39m calculate_bleu(references, translations)\n",
      "Cell \u001b[1;32mIn[3], line 24\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, iterator, target_tokenizer, device)\u001b[0m\n\u001b[0;32m     21\u001b[0m ref_seq \u001b[38;5;241m=\u001b[39m trg[i]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Convert to text\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m pred_text \u001b[38;5;241m=\u001b[39m target_tokenizer\u001b[38;5;241m.\u001b[39mdecode(pred_seq)\n\u001b[0;32m     25\u001b[0m ref_text \u001b[38;5;241m=\u001b[39m target_tokenizer\u001b[38;5;241m.\u001b[39mdecode(ref_seq)\n\u001b[0;32m     27\u001b[0m translations\u001b[38;5;241m.\u001b[39mappend(pred_text)\n",
      "File \u001b[1;32mc:\\Users\\yashv\\anaconda3\\Lib\\site-packages\\sentencepiece\\__init__.py:790\u001b[0m, in \u001b[0;36mSentencePieceProcessor.Decode\u001b[1;34m(self, input, out_type, num_threads)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_threads \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(num_threads) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    788\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_threads must be int\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[0;32m    791\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "def calculate_bleu(references, translations):\n",
    "    smoothie = SmoothingFunction().method1\n",
    "    bleu_scores = []\n",
    "    \n",
    "    for ref, trans in zip(references, translations):\n",
    "        score = sentence_bleu([ref.split()], trans.split(), smoothing_function=smoothie)\n",
    "        bleu_scores.append(score)\n",
    "    \n",
    "    return np.mean(bleu_scores)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Hyperparameters (must match training)\n",
    "    HIDDEN_DIM = 256\n",
    "    N_LAYERS = 2\n",
    "    DROPOUT = 0.3\n",
    "    VOCAB_SIZE = 8000  # Must match training vocabulary size\n",
    "    BATCH_SIZE = 64\n",
    "    \n",
    "    # Load data\n",
    "    train_df, val_df = load_data('english_assamese.csv')\n",
    "    source_tokenizer, target_tokenizer = create_tokenizers(train_df, vocab_size=VOCAB_SIZE)\n",
    "    \n",
    "    # Get vocabulary sizes\n",
    "    INPUT_DIM = source_tokenizer.get_piece_size()\n",
    "    OUTPUT_DIM = target_tokenizer.get_piece_size()\n",
    "    \n",
    "    # Initialize model\n",
    "    enc = Encoder(INPUT_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
    "    dec = Decoder(OUTPUT_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
    "    model = Seq2Seq(enc, dec)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Load model state\n",
    "    checkpoint_path = os.path.join('checkpoints', 'best_model_attention.pth')\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, weights_only=True)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        print(\"Model loaded successfully from checkpoint\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Checkpoint not found at {checkpoint_path}\")\n",
    "    \n",
    "    # Create validation dataset and loader\n",
    "    val_dataset = TranslationDataset(\n",
    "        val_df['eng'].tolist(),\n",
    "        val_df['asm'].tolist(),\n",
    "        source_tokenizer,\n",
    "        target_tokenizer\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        pin_memory=True if device.type == 'cuda' else False\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    translations, references, attention_maps = evaluate_model(model, val_loader, target_tokenizer, device)\n",
    "    \n",
    "    # Calculate BLEU score\n",
    "    bleu_score = calculate_bleu(references, translations)\n",
    "    print(f\"Average BLEU score: {bleu_score:.4f}\")\n",
    "    \n",
    "    # Print some example translations\n",
    "    print(\"\\nExample Translations:\")\n",
    "    for i in range(min(5, len(translations))):\n",
    "        print(f\"Source: {val_df['eng'].iloc[i]}\")\n",
    "        print(f\"Reference: {references[i]}\")\n",
    "        print(f\"Translation: {translations[i]}\")\n",
    "        print()\n",
    "    \n",
    "    # Plot attention maps for examples\n",
    "    if attention_maps:\n",
    "        for i, (source, target, attention) in enumerate(attention_maps[:5]):\n",
    "            plot_attention(torch.tensor(attention), source, target, source_tokenizer, target_tokenizer)\n",
    "            print(f\"Saved attention map for example {i+1}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94bd787",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
